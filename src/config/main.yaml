# CONFIG VALUES FOR RL FRAMEWORK.
##########################
# Contains all config values except hyperparameters for algorithms which
# is logged in the directories experiments (mlflow) and results

# Benefits of hydra logging
# - Make experiments reproducible by saving exact config values for each experiment
#   in an organized way.
# - Change values in user friendly terminal setup (tab completion) without touching the code
# - One organized place to change all config values, without the need to traverse the code
#   for hard coded values. This makes code use easier for team
# - Change a config value one place and not in a number of scripts (where you hopefully find all)
# - Remove several lines of comments in the code.
# - A hierachical configuration make it possible to easy change different parameters
#   for different agents and reward-experiments

defaults: # sub-configs
  - agent: ppo_best.yaml # ppo_best.yaml, ppo_default.yaml
  - REWARD: experiment_1.yaml # different setups for different sites and experiments
  - _self_ # main overrides sub-cfg in case of duplicates

hydra:
  run:
    dir: ./experiments/hydra_outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ./experiments/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

# TBM EXCAVATION PARAMETERS
###########################
TBM:
  CUTTERHEAD_RADIUS: 4 # cutterhead radius [m]
  TRACK_SPACING: 0.1 # cutter track spacing [m]
  LIFE: 400000 # theoretical durability of one cutter [m]
  STROKE_LENGTH: 1.8 # length of one stroke [m]
  MAX_STROKES: 1000 # number of strokes per episode

# MAIN EXPERIMENT INFO AND GENERAL PARAMETERS
######################
EXP:
  # MODE determines if either an optimization should run = "Optimization", or a
  # new agent is trained with prev. optimized parameters = "Training", or an
  # already trained agent is executed = "Execution"
  MODE: "training" # 'optimization', 'training', 'execution'
  # set to run SB3 environment check function
  # Checks if env is a suitable gym environment
  CHECK_ENV: False
  DEBUG: False
  STUDY: ${agent.NAME}_${now:%Y-%m-%d}_study # name of optuna study object.
  # evaluations in optimization and checkpoints in training every X episodes
  CHECKPOINT_INTERVAL: ${agent.CHECKPOINT_INTERVAL} # different for each agent
  EPISODES: ${agent.EPISODES} # max episodes to train for
  PLOT_PROGRESS: True # wether to make progress plots during training or not. Plotting takes time
  # deterministic or stocastic actions in evaluate_policy. SB3 tutorials use True
  # deterministic is also used to set this in execute for PPO and A2C. Ref: https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html
  DETERMINISTIC: True
  SEED_ALGORITHM: 42 # 42 or null - set seed as input to SB3 algorithm
  SEED_ALL: 42 # 42 or null - set seed using general np.seed torch.seed etc.

# OPTIMIZATION SPECIAL SETUP
######################
OPT:
  DEFAULT_TRIAL: False # first run a trial with default parameters.
  MAX_NO_IMPROVEMENT_EVALS: ${agent.MAX_NO_IMPROVEMENT_EVALS} # maximum number of evaluations without improvement
  # n optuna trials to run in total (including eventual default trial)
  N_SINGLE_RUN_OPTUNA_TRIALS: 100
  # NOTE: memory can be an issue for many parallell processes. Size of neural network and
  # available memory will be limiting factors
  N_CORES_PARALLELL: 1
  N_PARALLELL_PROCESSES: 1
  N_EVAL_EPISODES_OPTIMIZATION: 5 # n eval episodes in stop training cb. SB3 recommend 5-20
  N_EVAL_EPISODES_REWARD: 10 # n eval episodes for computing mean reward in end of training of one episode
  STUDYS:
  - PPO_2022_09_27_study
  - DDPG_2022_10_03_study
  - TD3_2022_09_27_study
  - A2C_2022_11_30_study
  - SAC_2022_10_05_study
  AGENTS:
  - PPO
  - DDPG
  - TD3
  - A2C
  - SAC
  BEST_PERFORMING_ALGORITHM_PATH: "/mnt/P/2022/00/20220043/Calculations/TD3_2022_09_27_study/TD3_e86e3a34-64dc-4b61-84ca-46d504c7851a"

# TRAINING SPECIAL SETUP
######################
TRAIN:
  # load best parameters from study object in training. Alternative: load from yaml
  LOAD_PARAMS_FROM_STUDY: False
  N_EVAL_EPISODES_TRAINING: 10
  N_DUPLICATES: 1  # Train on same config parameters x times. That is relevant in RL

# EXECUTION SPECIAL SETUP
######################
EXECUTE:
  EXECUTION_MODEL: "PPO_0ce28d43-982e-400e-8bf6-f29829d12ea1" # directory name with saved model in checkpoints or optimization
  NUM_TEST_EPISODES: 3
  VISUALIZE_STATE_ACTION_PLOT: False


# LAYOUT SETUP
######################
PLOT:
  FIGURE_WIDTH: 3.15
  DATA_DIR: "/mnt/P/2022/00/20220043/Calculations/"  # checkpoints, optimizations, or some other dir
  AGENT_NAME: TD3  # "PPO", "DDPG", "A2C", "TD3", "SAC", "PPO-LSTM"
  STUDY_NAME: TD3_2022_09_27_study
  MAKE_MAX_REWARD_LIST: False  # make a list of max reward for all experiments for each algorithm
  VISUALIZATION_MODE: rollout  # rollout or eval
  PRINT_TRESH: null  # null or int. Only print curves with reward over this threshold
  CHOOSE_NUM_BEST_REWARDS: null # null or int
  PLOTS_TO_MAKE: [
    # "training_path_experiments_single_algorithm",
    # "training_path_experiments_algorithms",
    "training_progress_status_plot",
    # "parallell_coordinate_plot",
    # "slice_plot",
    # "optimization_history_plot",
  ]

rich_console:
