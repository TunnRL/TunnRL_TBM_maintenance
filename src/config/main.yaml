# CONFIG VALUES FOR RL FRAMEWORK.
##########################
# Contains all config values except hyperparameters for algorithms which
# is logged in the directories experiments (mlflow) and results

# Benefits of hydra logging
# - Make experiments reproducible by saving exact config values for each experiment
#   in an organized way.
# - Change values in user friendly terminal setup (tab completion) without touching the code
# - One organized place to change all config values, without the need to traverse the code
#   for hard coded values. This makes code use easier for team
# - Change a config value one place and not in a number of scripts (where you hopefully find all)
# - Remove several lines of comments in the code.
# - A hierachical configuration make it possible to easy change different parameters
#   for different agents and reward-experiments

defaults: # sub-configs
  - agent: a2c.yaml
  - REWARD: experiment_1.yaml # different setups for different sites and experiments
  - _self_ # main overrides sub-cfg in case of duplicates
  - base_config

hydra:
  run:
    dir: ./experiments/hydra_outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ./experiments/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

# TBM EXCAVATION PARAMETERS
###########################
TBM:
  CUTTERHEAD_RADIUS: 4 # cutterhead radius [m]
  TRACK_SPACING: 0.1 # cutter track spacing [m]
  LIFE: 400000 # theoretical durability of one cutter [m]
  STROKE_LENGTH: 1.8 # length of one stroke [m]
  MAX_STROKES: 1000 # number of strokes per episode

# MAIN EXPERIMENT INFO AND GENERAL PARAMETERS
######################
EXP:
  # MODE determines if either an optimization should run = "Optimization", or a
  # new agent is trained with prev. optimized parameters = "Training", or an
  # already trained agent is executed = "Execution"
  MODE: "optimization" # 'optimization', 'training', 'execution'
  # set to run SB3 environment check function
  # Checks if env is a suitable gym environment
  CHECK_ENV: False
  DEBUG: False
  STUDY: ${agent.NAME}_${now:%Y-%m-%d}_study # name of optuna study object.
  # evaluations in optimization and checkpoints in training every X episodes
  CHECKPOINT_INTERVAL: ${agent.CHECKPOINT_INTERVAL} # different for each agent
  EPISODES: 12_000 # max episodes to train for
  PLOT_PROGRESS: True # wether to make progress plots during training or not. Plotting takes time
  DETERMINISTIC: False # fixed environment used in evaluation or not

# OPTIMIZATION SPECIAL SETUP
######################
OPT:
  DEFAULT_TRIAL: False # first run a trial with default parameters.
  MAX_NO_IMPROVEMENT_EVALS: ${agent.MAX_NO_IMPROVEMENT_EVALS} # maximum number of evaluations without improvement
  # n optuna trials to run in total (including eventual default trial)
  N_SINGLE_RUN_OPTUNA_TRIALS: 100
  # NOTE: memory can be an issue for many parallell processes. Size of neural network and
  # available memory will be limiting factors
  N_CORES_PARALLELL: -1
  N_PARALLELL_PROCESSES: 1
  N_EVAL_EPISODES_OPTIMIZATION: 3 # n eval episodes in stop training cb
  N_EVAL_EPISODES_REWARD: 10 # n eval episodes for computing mean reward in end of training

# TRAINING SPECIAL SETUP
######################
TRAIN:
  # load best parameters from study object in training. Alternative: load from yaml
  LOAD_PARAMS_FROM_STUDY: False
  N_EVAL_EPISODES_TRAINING: 10

# EXECUTION SPECIAL SETUP
######################
EXECUTE:
  EXECUTION_MODEL: "" # directory name with saved model in checkpoints or optimization
  NUM_TEST_EPISODES: 3
  VISUALIZE_EPISODES: False
